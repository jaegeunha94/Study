# AI
* [수술](https://www.youtube.com/shorts/kx89BN7uMpw)
* [AI 무기](https://www.youtube.com/shorts/F4yBl1DBYVk)
* [Suno v4](https://www.youtube.com/shorts/tZAYHPtOVrY)
  * [Suno 특징](https://www.threads.net/@qjc.ai/post/DClmDHRSWI9)
  * [버클리 음대 교슈](https://www.threads.net/@choi.openai/post/DC2lWSzBsMz/%EB%B2%84%ED%81%B4%EB%A6%AC-%EC%9D%8C%EB%8C%80-%EA%B5%90%EC%88%98-ai%EA%B0%80-%EB%82%B4-%ED%95%99%EC%83%9D%EB%93%A4%EC%9D%98-80-%EB%B3%B4%EB%8B%A4-%EB%82%AB%EB%8B%A4%EC%A0%84%EB%AC%B8%EA%B0%80%EC%9E%84-1%EC%96%B5-%EC%8A%A4%ED%8A%B8%EB%A6%AC%EB%B0%8D-%EA%B3%A8%EB%93%9C-%EB%A0%88%EC%BD%94%EB%93%9C-%EB%B2%84%ED%81%B4%EB%A6%AC-%EC%9D%8C%EC%95%85%EB%8C%80%ED%95%99%EC%97%90%EC%84%9C-10%EB%85%84%EA%B0%84-%EC%9E%91%EA%B3%A1%EC%9D%84-%EA%B0%80%EB%A5%B4%EC%B3%A4%EC%9D%8C%EC%9D%8C%EC%95%85%EC%A0%81%EC%9C%BC%EB%A1%9C%EB%8A%94-)
* [양자 컴퓨터](https://www.youtube.com/shorts/KKozGhRPfXQ)
* [클로드 글쓰기 스타일](https://tilnote.io/pages/67466cb4d1b33d8b8004da9a)
  * [스타일](https://www.threads.net/@nodaji_ai/post/DC8KAVwOMSU)
* [클로드 구글 docs](https://www.threads.net/@nodaji_ai/post/DCxtGREygBn)
* [클로드 Improve Prompt](https://www.threads.net/@nodaji_ai/post/DCxsuGOyhKP)
* [AWS Claude 투자](https://tilnote.io/pages/67411b1dd1b33d8b80009bc0)
  * [투자 현황](https://www.threads.net/@unclejobs.ai/post/DCtx8i0zq73)
* [아마존 AI 칩](https://www.threads.net/@eunho.letter/post/DCz0c48TbvU)
* [Claude 업데이트 전체](https://www.threads.net/@choi.openai/post/DC70jephdy7)
* [AI 의사](https://www.threads.net/@gpt_minje/post/DC5lrC2IsJb)
* [옵티머스](https://www.threads.net/@choi.openai/post/DC6q1JxBXEh)
   * [테슬라 옵티머스](https://namu.wiki/w/%ED%85%8C%EC%8A%AC%EB%9D%BC%20%EC%98%B5%ED%8B%B0%EB%A8%B8%EC%8A%A4)
* [MCP](https://www.threads.net/@aicoffeechat/post/DC0R8yYzmlV)
* [Custom Instructions](https://www.threads.net/@choi.openai/post/DC6jVEhB01q)

# RAG
* [RAG Report](https://github.com/hkhwang731/HairLossProject/tree/main/Report/RAG)

# 평가 Dataset
## 작지만 고품질로 만드는게 좋다

## LLM으로 평가 데이터셋을 만들때는 가장 똑똑한 모델로 만들어야 한다

## 데이터가 변경될 때마다 RAG 평가는 계속 되야 한다.

## 좋은 RAG 평가 데이터 만드는 방법
* 데이터 생성을 전부 자동화하는 것은 거의 불가능
* 가장 좋은 방법은 실제 유저 데이터 활용
* 두번째로는 '도메인 전문가와 만드는 것이
* 세 번째 옵션이 바로 human-in-the-loop 전략.
* 평가 데이터셋 만드는 일은 고통스럽지만, 반드시 많은 시간과 노력을 투입해야 함.(GIGO)

# Retrieval 지표
이미지에 있는 **Retrieval 지표**에 대해 설명드리겠습니다. Retrieval 지표는 검색 시스템이나 정보 검색(IR)에서 쿼리에 대해 얼마나 적절하게 문서나 정보를 찾아냈는지 평가하는 기준입니다. 각각의 지표는 검색 시스템의 성능을 측정하는 데 사용되며, 검색된 결과의 정확성, 순서, 포함된 정답의 비율 등을 평가합니다.

## 1. **Retrieval Precision**
- 예를 들어, Retrieval의 k를 10을 설정해서 답변이 그 안에 모두 있어서 recall은 100%, 근데 이때 답변에 필요한 문서가 상위 랭크 (점수)에 들어와 있어야 한다.
- **LLM의 context length가 적다면 중요**: 검색된 결과에서 얼마나 많은 정답이 포함되었는지를 평가하는 지표입니다. LLM(대형 언어 모델)의 컨텍스트 길이가 제한적일 때, 검색된 문서 중에 얼마나 많은 문서가 정확한지(정답인지)를 측정하는 것이 중요합니다.
  
- **환각 증세를 줄이고 싶을 때 유용**: 검색 시스템이 잘못된 문서(정답이 아닌 문서)를 반환하는 '환각' 현상을 줄이고 싶을 때 이 지표가 유용합니다. 즉, 정답이 아닌 문서를 덜 포함시킬수록 Precision이 높아집니다.

## 2. **Retrieval Recall**
* 예를 들어, 요청을 3가지를 했는데 검색된 Context가 이 3가지에 충족해야함
- **대부분의 경우에 유용, 이해하기 쉬움**: 검색된 결과 중에 정답이 얼마나 포함되었는지를 평가합니다. 쿼리와 관련된 전체 문서 중 검색된 문서의 비율을 측정합니다.

- **만약 recall이 0.5라면, retrieval gt 단락 중 50%를 retrieve 하지 못한 것**: Recall이 0.5라는 것은 전체 정답의 절반만 검색되었음을 의미합니다. 즉, 검색된 문서 중에 여전히 많은 정답이 누락되었을 수 있음을 나타냅니다.

## 3. **Retrieval F1**
- **Retrieval precision을 사용하고 싶지 않다면 사용하지 않기**: F1 점수는 Precision과 Recall을 모두 고려한 조화 평균입니다. Precision과 Recall의 균형을 맞추고자 할 때 유용합니다.
  
- **Recall과 precision의 조화평균**: 검색된 문서의 정확도(Precision)와 검색된 문서 중 정답이 얼마나 포함되었는지(Recall)를 동시에 평가하는 지표입니다. Precision과 Recall 중 어느 한쪽으로 치우치지 않도록 하는 것이 특징입니다.

## 4. **NDCG (Normalized Discounted Cumulative Gain)**
- **순서를 고려하는 지표**: NDCG는 검색된 문서의 순서에 따라 가중치를 부여하는 방식입니다. 검색된 결과가 사용자의 쿼리에 맞게 중요한 문서를 상위에 배치했는지를 평가합니다.
  
- **직관적으로 이해하기는 어렵지만, 모델 간의 성능을 비교하는 데 유용함**: NDCG는 각 문서의 순위에 따라 다른 가중치를 부여하기 때문에, 직관적으로 이해하기는 어려울 수 있지만 검색 모델의 성능을 비교할 때 유용합니다.

## 5. **mAP (mean Average Precision)**
- **순서를 고려하는 지표, 이해하기 쉬움**: mAP는 검색된 결과에서 여러 쿼리에 대해 평균 Precision을 계산하는 지표입니다. 검색된 결과의 순서가 중요한 상황에서 사용할 수 있습니다.
  
- **만약 mAP가 0.2라면, 평균적으로 5개의 단락을 retrieve하면 정답이 포함되어 있다는 뜻**: mAP 값이 0.2라는 것은 평균적으로 5개의 문서를 검색하면 그 중 하나가 정답일 확률이 높다는 의미입니다.

## 6. **mRR (mean Reciprocal Rank)**
- **순서를 고려하는 지표, 같은 뜻을 가진 단락이 여러 개 있을 때 유용함**: mRR은 검색된 결과에서 첫 번째 정답의 위치(순위)를 평가하는 지표입니다. Reciprocal Rank는 첫 번째 정답의 순위를 역수로 변환하여 계산합니다.
  
- **정답이 여러 개 있을 때 유용함**: 같은 뜻을 가진 정답이 여러 개 있을 때, 그 중 가장 상위에 위치한 정답이 얼마나 빠르게 검색되었는지를 평가하는 데 유용합니다.

### 요약
- **Retrieval Precision**: 검색된 문서 중 정답이 얼마나 포함되었는지(정확도) 평가.
- **Retrieval Recall**: 전체 정답 중 얼마나 많이 검색되었는지 평가.
- **Retrieval F1**: Precision과 Recall의 균형을 평가.
- **NDCG**: 검색된 문서의 순서가 얼마나 적절한지 평가.
- **mAP**: 여러 쿼리에 대한 평균 Precision을 계산하는 지표로, 순서에 따라 결과를 평가.
- **mRR**: 첫 번째 정답이 얼마나 상위에 위치하는지 평가.

# 모델 튜닝
## 1. **Fine-Tuning (세부 조정)**
Fine-Tuning은 이미 학습된 언어 모델을 특정 작업에 맞게 추가로 학습시키는 과정입니다. 이는 대형 언어 모델이 이미 방대한 데이터로 사전 학습을 마친 상태에서, 특정 도메인이나 과업에 맞는 데이터셋을 사용하여 모델을 다시 학습시킴으로써 더 정밀한 예측이나 문장 생성을 가능하게 합니다. Fine-Tuning을 통해 모델은 일반적인 언어 이해를 넘어서 특정 분야의 용어, 문맥, 스타일 등을 학습할 수 있습니다.

## 2. **Prompt Engineering (프롬프트 엔지니어링)**
Prompt Engineering은 언어 모델에 입력하는 프롬프트(질문이나 요청 문장)를 최적화하여 원하는 결과를 얻는 기술입니다. 프롬프트의 구조나 어휘를 조정하여 모델이 더 정확한 답변을 생성하도록 유도합니다. 예를 들어, 프롬프트에 추가적인 맥락을 제공하거나 명확한 지시를 추가함으로써 모델이 의도한 방향으로 응답할 수 있도록 합니다. 이는 모델 자체를 변경하지 않고도 성능을 개선할 수 있는 방법으로, 빠르게 적용할 수 있으며 다양한 분야에서 유용하게 사용됩니다.

## 3. **Feature-Tuning (특성 조정)**
Feature-Tuning은 모델의 특정 기능을 조정하여 성능을 개선하는 방법입니다. 이는 모델의 가중치나 파라미터를 조정하는 것뿐만 아니라, 입력 데이터의 특정 특징을 강조하거나 제거함으로써 모델의 성능을 조절하는 과정도 포함됩니다. Feature-Tuning은 주로 모델의 출력 결과가 특정 요인에 크게 영향을 받을 때 그 요인을 통제하고자 할 때 사용됩니다.

각각의 튜닝 방법은 대형 언어 모델의 성능을 최적화하고, 특정 사용 사례에 맞게 모델을 조정하는 데 중요한 역할을 합니다.  
Fine-Tuning은 모델을 더 전문화된 지식으로 무장시키는 반면, Prompt Engineering과 Feature-Tuning은 이미 학습된 모델을 특정 맥락에서 더 효과적으로 사용하도록 돕습니다.  
이를 통해 LLM은 다양한 응용 프로그램과 도메인에서 더 나은 성능을 발휘할 수 있습니다.

## RAG와 Finetuning 적용할 때 시점
- 지식 참조나 정보 주입은 RAG
- 답변 스타일 / 답변 형식 / 말투 변경은 FineTuning 