
# RAG
* [RAG Report](https://github.com/hkhwang731/HairLossProject/tree/main/Report/RAG)

# 평가 Dataset
## 작지만 고품질로 만드는게 좋다

## LLM으로 평가 데이터셋을 만들때는 가장 똑똑한 모델로 만들어야 한다

## 데이터가 변경될 때마다 RAG 평가는 계속 되야 한다.

## 좋은 RAG 평가 데이터 만드는 방법
* 데이터 생성을 전부 자동화하는 것은 거의 불가능
* 가장 좋은 방법은 실제 유저 데이터 활용
* 두번째로는 '도메인 전문가와 만드는 것이
* 세 번째 옵션이 바로 human-in-the-loop 전략.
* 평가 데이터셋 만드는 일은 고통스럽지만, 반드시 많은 시간과 노력을 투입해야 함.(GIGO)

# Retrieval 지표
이미지에 있는 **Retrieval 지표**에 대해 설명드리겠습니다. Retrieval 지표는 검색 시스템이나 정보 검색(IR)에서 쿼리에 대해 얼마나 적절하게 문서나 정보를 찾아냈는지 평가하는 기준입니다. 각각의 지표는 검색 시스템의 성능을 측정하는 데 사용되며, 검색된 결과의 정확성, 순서, 포함된 정답의 비율 등을 평가합니다.

## 1. **Retrieval Precision**
- 예를 들어, Retrieval의 k를 10을 설정해서 답변이 그 안에 모두 있어서 recall은 100%, 근데 이때 답변에 필요한 문서가 상위 랭크 (점수)에 들어와 있어야 한다.
- **LLM의 context length가 적다면 중요**: 검색된 결과에서 얼마나 많은 정답이 포함되었는지를 평가하는 지표입니다. LLM(대형 언어 모델)의 컨텍스트 길이가 제한적일 때, 검색된 문서 중에 얼마나 많은 문서가 정확한지(정답인지)를 측정하는 것이 중요합니다.
  
- **환각 증세를 줄이고 싶을 때 유용**: 검색 시스템이 잘못된 문서(정답이 아닌 문서)를 반환하는 '환각' 현상을 줄이고 싶을 때 이 지표가 유용합니다. 즉, 정답이 아닌 문서를 덜 포함시킬수록 Precision이 높아집니다.

## 2. **Retrieval Recall**
* 예를 들어, 요청을 3가지를 했는데 검색된 Context가 이 3가지에 충족해야함
- **대부분의 경우에 유용, 이해하기 쉬움**: 검색된 결과 중에 정답이 얼마나 포함되었는지를 평가합니다. 쿼리와 관련된 전체 문서 중 검색된 문서의 비율을 측정합니다.

- **만약 recall이 0.5라면, retrieval gt 단락 중 50%를 retrieve 하지 못한 것**: Recall이 0.5라는 것은 전체 정답의 절반만 검색되었음을 의미합니다. 즉, 검색된 문서 중에 여전히 많은 정답이 누락되었을 수 있음을 나타냅니다.

## 3. **Retrieval F1**
- **Retrieval precision을 사용하고 싶지 않다면 사용하지 않기**: F1 점수는 Precision과 Recall을 모두 고려한 조화 평균입니다. Precision과 Recall의 균형을 맞추고자 할 때 유용합니다.
  
- **Recall과 precision의 조화평균**: 검색된 문서의 정확도(Precision)와 검색된 문서 중 정답이 얼마나 포함되었는지(Recall)를 동시에 평가하는 지표입니다. Precision과 Recall 중 어느 한쪽으로 치우치지 않도록 하는 것이 특징입니다.

## 4. **NDCG (Normalized Discounted Cumulative Gain)**
- **순서를 고려하는 지표**: NDCG는 검색된 문서의 순서에 따라 가중치를 부여하는 방식입니다. 검색된 결과가 사용자의 쿼리에 맞게 중요한 문서를 상위에 배치했는지를 평가합니다.
  
- **직관적으로 이해하기는 어렵지만, 모델 간의 성능을 비교하는 데 유용함**: NDCG는 각 문서의 순위에 따라 다른 가중치를 부여하기 때문에, 직관적으로 이해하기는 어려울 수 있지만 검색 모델의 성능을 비교할 때 유용합니다.

## 5. **mAP (mean Average Precision)**
- **순서를 고려하는 지표, 이해하기 쉬움**: mAP는 검색된 결과에서 여러 쿼리에 대해 평균 Precision을 계산하는 지표입니다. 검색된 결과의 순서가 중요한 상황에서 사용할 수 있습니다.
  
- **만약 mAP가 0.2라면, 평균적으로 5개의 단락을 retrieve하면 정답이 포함되어 있다는 뜻**: mAP 값이 0.2라는 것은 평균적으로 5개의 문서를 검색하면 그 중 하나가 정답일 확률이 높다는 의미입니다.

## 6. **mRR (mean Reciprocal Rank)**
- **순서를 고려하는 지표, 같은 뜻을 가진 단락이 여러 개 있을 때 유용함**: mRR은 검색된 결과에서 첫 번째 정답의 위치(순위)를 평가하는 지표입니다. Reciprocal Rank는 첫 번째 정답의 순위를 역수로 변환하여 계산합니다.
  
- **정답이 여러 개 있을 때 유용함**: 같은 뜻을 가진 정답이 여러 개 있을 때, 그 중 가장 상위에 위치한 정답이 얼마나 빠르게 검색되었는지를 평가하는 데 유용합니다.

### 요약
- **Retrieval Precision**: 검색된 문서 중 정답이 얼마나 포함되었는지(정확도) 평가.
- **Retrieval Recall**: 전체 정답 중 얼마나 많이 검색되었는지 평가.
- **Retrieval F1**: Precision과 Recall의 균형을 평가.
- **NDCG**: 검색된 문서의 순서가 얼마나 적절한지 평가.
- **mAP**: 여러 쿼리에 대한 평균 Precision을 계산하는 지표로, 순서에 따라 결과를 평가.
- **mRR**: 첫 번째 정답이 얼마나 상위에 위치하는지 평가.
